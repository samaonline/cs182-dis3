{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DIS 3] ReLUs and Neural Network Intuition with different Optimizers\n",
    "\n",
    "**If you are running on a local anaconda install, you will need to install pytorch with the command**\n",
    "```sh\n",
    "conda install pytorch -c pytorch\n",
    "```\n",
    "\n",
    "<!-- #TODO(krishna) : add wandb integration -->\n",
    "<!-- You should immediately run all the cells up through 'Train all layers' since training the networks takes a long time. While you wait you can return to the theory portion of this discussion and work on the backpropagation problem. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from ipywidgets import fixed, interactive, widgets \n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training and Test Data\n",
    "\n",
    "We are using piecewise linear function. Our training data has added noise $y = f(x) + \\epsilon,\\, \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The test data is noise free.\n",
    "\n",
    "_Once you have gone through the discussion once you may wish to adjust the number of training samples and noise variance to see how gradient descent behaves under the new conditions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_type = 'piecewise_linear'\n",
    "\n",
    "def f_true(X, f_type):\n",
    "    if f_type == 'sin(20x)':\n",
    "        return np.sin(20 * X[:,0])\n",
    "    else:\n",
    "        TenX = 10 * X[:,0]\n",
    "        _ = 12345\n",
    "        return (TenX - np.floor(TenX)) * np.sin(_ * np.ceil(TenX)) - (TenX - np.ceil(TenX)) * np.sin(_ * np.floor(TenX)) \n",
    "    \n",
    "n_features = 1\n",
    "n_samples = 200\n",
    "sigma = 0.01\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "# Generate train data\n",
    "X = np.sort(rng.rand(n_samples, n_features), axis=0)\n",
    "y = f_true(X, f_type) + rng.randn(n_samples) * sigma\n",
    "\n",
    "# Generate NOISELESS test data\n",
    "X_test = np.concatenate([X.copy(), np.expand_dims(np.linspace(0., 1., 1000), axis=1)])\n",
    "X_test = np.sort(X_test, axis=0)\n",
    "y_test = f_true(X_test, f_type)\n",
    "\n",
    "# Save checkpoint files\n",
    "DIR_SGD = os.path.join(os.getcwd(), 'ckpts/sgd')\n",
    "DIR_SGDM = os.path.join(os.getcwd(), 'ckpts/sgd_momentum')\n",
    "DIR_ADAM = os.path.join(os.getcwd(), 'ckpts/adam')\n",
    "os.makedirs(DIR_SGD, exist_ok=True)\n",
    "os.makedirs(DIR_SGDM, exist_ok=True)\n",
    "os.makedirs(DIR_ADAM, exist_ok=True)\n",
    "\n",
    "def get_ckpt_dir(optim: str):\n",
    "    if optim == 'sgd':\n",
    "        return DIR_SGD\n",
    "    elif optim == 'sgd_momentum':\n",
    "        return DIR_SGDM\n",
    "    elif optim == 'adam':\n",
    "        return DIR_ADAM\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Networks\n",
    "\n",
    "We will learn the piecewise linear target function using a simple 1-hidden layer neural network with ReLU non-linearity, defined by\n",
    "$$ \\hat{y} = \\mathbf{W}^{(2)} \\Phi \\left( \\mathbf{W}^{(1)} x + \\mathbf{b}^{(1)} \\right) + \\mathbf{b}^{(2)} $$\n",
    "where $\\Phi(x) = ReLU(x)$ and superscripts refer to indices, not the power operator.\n",
    "\n",
    "We will also create two SGD optimizers to allow us to choose whether to train all parameters or only the linear output layer's parameters. Note that we use separate learning rates for the two version of training. There is too much variance in the gradients when training all layers to use a large learning rate, so we have to decrease it.\n",
    "\n",
    "We will modify the default initialization of the biases so that the ReLU elbows are all inside the region we are interested in.\n",
    "\n",
    "We create several versions of this network with varying widths to explore how hidden layer width impacts learning performance.\n",
    "\n",
    "_Once you have gone through the discussion once you may wish to train networks with even larger widths to see how they behave under the three different training paradigms in this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't rerun this cell after training or you will lose all your work\n",
    "nets_by_size = {}\n",
    "nn_widths = [10, 20, 40]\n",
    "nn_optimizer = ['sgd', \"sgd_momentum\", 'adam']\n",
    "nn_seeds = [\n",
    "  442, 370, 378, 892, 836, 209, 327, 316, 216, 308,\n",
    "  748, 934, 558, 546, 266, 808, 884, 818, 277, 979, \n",
    "  766, 274, 479, 325, 431, 971, 689, 871, 272, 704\n",
    "]\n",
    "\n",
    "def setup_networks(widths, optimizer, seed):\n",
    "  \n",
    "  torch.manual_seed(seed)\n",
    "  nets_by_size[seed] = dict()\n",
    "\n",
    "  for width in widths:\n",
    "\n",
    "      nets_by_size[seed][width] = dict()\n",
    "      # Define a 1-hidden layer ReLU nonlinearity network. \n",
    "      # Initialize outside the optimizer loop to keep weight init the same.\n",
    "      net = nn.Sequential(\n",
    "        nn.Linear(1, width),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(width, 1)\n",
    "      )\n",
    "\n",
    "      for optim in optimizer:\n",
    "        \n",
    "        # Clone the network\n",
    "        network = copy.deepcopy(net)\n",
    "\n",
    "        # Get trainable parameters\n",
    "        weights_all = list(network.parameters())\n",
    "        \n",
    "        # Get the output weights alone\n",
    "        weights_out = weights_all[2:]\n",
    "        # Adjust initial biases so elbows are in [0,1]\n",
    "        elbows = np.sort(np.random.rand(width)) \n",
    "        new_biases = -elbows * to_numpy(weights_all[0].cpu()).ravel()\n",
    "        weights_all[1].data = to_torch(new_biases)\n",
    "        # Create SGD optimizers for outputs alone and for all weights\n",
    "        # lr_out = 0.2\n",
    "        lr_all = 0.02\n",
    "        if optim == 'sgd':\n",
    "          opt_all = torch.optim.SGD(params=weights_all, lr=lr_all)\n",
    "        elif optim == 'sgd_momentum':\n",
    "          opt_all = torch.optim.SGD(params=weights_all, lr=lr_all, momentum=0.9)\n",
    "        elif optim == 'adam':\n",
    "          opt_all = torch.optim.Adam(params=weights_all, lr=lr_all )\n",
    "        # opt_out = torch.optim.SGD(params=weights_out, lr=lr_out)\n",
    "        nets_by_size[seed][width][optim] = {\n",
    "          'net': network,\n",
    "          'opt_all': opt_all,\n",
    "          'optim': optim,\n",
    "          'seed': seed\n",
    "        }\n",
    "\n",
    "for s in nn_seeds:\n",
    "  setup_networks(nn_widths, nn_optimizer, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train all the networks now - this will take a while!\n",
    "You can expect training to take between 5 and 10 minutes depending on whether you run locally or on datahub and how heavily loaded datahub is at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Width 10 Optimizer sgd\n",
      "training with 30 seeds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [01:42<05:36, 14.64s/it]"
     ]
    }
   ],
   "source": [
    "n_steps = 30000\n",
    "save_every = 3000 #1000\n",
    "t0 = time.time()\n",
    "\n",
    "def train_all_seeds(widths, optims, seeds):\n",
    "  \n",
    "    for w in widths:\n",
    "      for i, optim in enumerate(optims):\n",
    "\n",
    "        print(\"-\"*40)\n",
    "        print(\"Width\", w, \"Optimizer\", optim)\n",
    "        list_of_history = []\n",
    "\n",
    "        print(f\"training with {len(seeds)} seeds...\")\n",
    "        for seed in tqdm(seeds):\n",
    "          net = nets_by_size[seed][w][optim]['net']\n",
    "          opt_all = nets_by_size[seed][w][optim]['opt_all']\n",
    "\n",
    "          save_dir = f'{get_ckpt_dir(optim)}/width{w}/seed{seed}/'\n",
    "          os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "          history_all = train_network(X, y, X_test, y_test, \n",
    "                                  net, optim=opt_all, \n",
    "                                  n_steps=n_steps, save_every=save_every, \n",
    "                                  verbose=False, optimizer=optim, seed=seed,\n",
    "                                  ckpt_dir=save_dir)\n",
    "          nets_by_size[seed][w][optim]['hist_all'] = history_all\n",
    "          list_of_history.append(history_all)  \n",
    "      \n",
    "train_all_seeds(widths=nn_widths, optims=nn_optimizer, seeds=nn_seeds)\n",
    "  \n",
    "t1 = time.time()\n",
    "print(\"-\"*40)\n",
    "print(\"Trained all layers in %.1f minutes\" % ((t1 - t0) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nn_widths:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, optim in enumerate(nn_optimizer):\n",
    "        \n",
    "        # uncomment below if you ran the previous block to train all models\n",
    "        # list_of_history = [nets_by_size[s][w][optim]['hist_all'] for s in nn_seeds]\n",
    "        \n",
    "        # uncomment below if you want to use pretrained weights\n",
    "        c = get_ckpt_dir(optim)\n",
    "        list_of_history = [\n",
    "            torch.load(\n",
    "                os.path.join(f'{get_ckpt_dir(optim)}/width{w}/seed{s}', 'ckpt_and_history.pt')\n",
    "            ) for s in nn_seeds\n",
    "        ]\n",
    "\n",
    "        print(\"-\"*40)\n",
    "        print(\"Width\", w, \"Optimizer\", optim)\n",
    "        plot_with_error_bar(list_of_history, optim=optim, plot_train=True, idx=i)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Test Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in nn_widths:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, optim in enumerate(nn_optimizer):\n",
    "\n",
    "        # uncomment below if you ran the previous block to train all models\n",
    "        # list_of_history = [nets_by_size[s][w][optim]['hist_all'] for s in nn_seeds]\n",
    "        \n",
    "        # uncomment below if you want to use pretrained weights\n",
    "        c = get_ckpt_dir(optim)\n",
    "        list_of_history = [\n",
    "            torch.load(\n",
    "                os.path.join(f'{get_ckpt_dir(optim)}/width{w}/seed{s}', 'ckpt_and_history.pt')\n",
    "            ) for s in nn_seeds\n",
    "        ]\n",
    "\n",
    "        print(\"-\"*40)\n",
    "        print(\"Width\", w, \"Optimizer\", optim)\n",
    "        plot_with_error_bar(list_of_history, optim=optim, plot_test=True, idx=i)  \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize ReLU elbow positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = nn_seeds[0]\n",
    "for w in nn_widths:\n",
    "    for optim in nn_optimizer:\n",
    "        net = nets_by_size[SAMPLE][w][optim]['net']\n",
    "        print(\"Width\", w)\n",
    "        plot_update(X, y, X_test, y_test, net, optim=optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Questions to consider while exploring the training process for all layer weights with different optimizers:\n",
    "- **How does the hidden layer width and different optimizers impact the learned function and test error?**\n",
    "\n",
    "- **What happens to the elbow locations using different optimizers during training?**\n",
    "- **Check out `helper.py` file. Are the circle dots on the graphs above mean or median? Why not plot the other one?**\n",
    "\n",
    "-  **How are error bars computed? Are the upper and lower marks maximum/minimum, standard deviation or something else? Does it make sense to plot standard deviation here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ae701bdce0aa397e58415e34d2e4c6d81a84f25aea1453482d23749666ca1c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
